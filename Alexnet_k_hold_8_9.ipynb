{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1nOZRoFnyJlI_XGKS1UTqSOSCPtZHgJgt",
      "authorship_tag": "ABX9TyOPsz9bN3cZ6UvM4kWtxMXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/multikokoiti1998/vgg_8_3/blob/k-fold/Alexnet_k_hold_8_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99cWF84SU3ZF",
        "outputId": "e9745e04-a469-4999-acdf-196c901159d7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.13)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.23.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.12.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.8.2)\n",
            "Requirement already satisfied: albucore>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.13)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from albucore>=0.0.13->albumentations) (2.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.20.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.7.24)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import albumentations as A"
      ],
      "metadata": {
        "id": "DULjDcR5raJU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像ディレクトリと画像パスの取得\n",
        "image_directory = '/content/drive/MyDrive/lernig/obiref88/image'\n",
        "label_directory = '/content/drive/MyDrive/lernig/obiref88/label'\n",
        "\n",
        "def update_xml_paths(image_directory: str, label_directory: str):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # 画像ファイルとXMLファイルの一致確認\n",
        "    for image_file in os.listdir(image_directory):\n",
        "        if image_file.endswith('.jpg'):\n",
        "            base_name = image_file.replace('.jpg', '')\n",
        "\n",
        "            xml_path = Path(label_directory) / f\"{base_name}.xml\"\n",
        "\n",
        "            if xml_path.exists():\n",
        "                tree = ET.parse(xml_path)\n",
        "                root = tree.getroot()\n",
        "                label = root.find('.//name')\n",
        "                image_paths.append(os.path.join(image_directory, image_file))\n",
        "                labels.append(label.text)\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "image_paths, labels = update_xml_paths(image_directory, label_directory)\n",
        "print(\"Number of image paths:\", len(image_paths))\n",
        "print(\"Number of labels:\", len(labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSd6zvhz4OVy",
        "outputId": "78aa6429-5159-416d-c81a-def93434515f",
        "collapsed": true
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image paths: 300\n",
            "Number of labels: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = {\n",
        "    'train': transforms.Compose(\n",
        "        [transforms.Resize((256, 256)),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "         transforms.RandomRotation(degrees=[-5, 5]),\n",
        "         transforms.ColorJitter(brightness=0.3, contrast=0.5),\n",
        "         #A.GaussNoise(var_limit=(10, 50))transformライブラリでないので変換前後で定義しなおさなくてはならない,\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.35187622, 0.35187622, 0.35187622],\n",
        "                              std=[0.33888655, 0.33888655, 0.33888655]),\n",
        "         ]),\n",
        "    'val': transforms.Compose(\n",
        "        [transforms.Resize((256, 256)),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.35187622, 0.35187622, 0.35187622],\n",
        "                              std=[0.33888655, 0.33888655, 0.33888655]),\n",
        "         ])\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "g7a7oKMerJE0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'ok': 0,\n",
        "    'out': 1\n",
        "}\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = [label_mapping[label] for label in labels]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        label = self.labels[index]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            label=torch.tensor(label)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "epfjOB6Q5eWa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3rNY9bwUI-YG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "train_dataset=[]\n",
        "val_dataset=[]\n",
        "total_train_accs=[]\n",
        "total_val_accs=[]\n",
        "data_size = {'train': len(train_dataset), 'val': len(val_dataset)}"
      ],
      "metadata": {
        "id": "qutqC-3s6jy5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs):\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "\n",
        "                running_loss = 0.0\n",
        "                corrects = 0\n",
        "\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                epoch_loss = running_loss / data_size[phase]\n",
        "                epoch_acc = corrects.double() / data_size[phase]\n",
        "\n",
        "                if phase == 'train':\n",
        "                    train_losses.append(epoch_loss)\n",
        "                    train_accs.append(epoch_acc)\n",
        "                    print('{}Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "                else:\n",
        "                    val_losses.append(epoch_loss)\n",
        "                    val_accs.append(epoch_acc)\n",
        "                    print('{}Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "              #print('{}Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        train_accs_cpu = [acc.cpu().numpy() for acc in train_accs]\n",
        "        val_accs_cpu = [acc.cpu().numpy() for acc in val_accs]\n",
        "\n",
        "        plot_training_history(train_losses, val_losses, train_accs_cpu, val_accs_cpu)\n",
        "\n",
        "\n",
        "\n",
        "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Lossのプロット\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracyのプロット\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accs, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accs, 'ro-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.ylim(0.4, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bMwCm7Zv5-qB"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = torchvision.models.alexnet(pretrained=True)\n",
        "\n",
        "#for param in net.parameters():\n",
        "          #param.requires_grad = False\n",
        "\n",
        "# モデルをGPUに移動\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = net.to(device)\n",
        "\n",
        "# 損失関数と最適化アルゴリズムを定義(MSE）\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "# モデルのトレーニングと評価\n",
        "model = net\n",
        "base_path = r'/content/drive/MyDrive/learning'\n",
        "folder_name = 'weight'\n",
        "directory_path = os.path.join(base_path, folder_name)\n",
        "file_name = 'alexnet.pth'\n",
        "file_path = os.path.join(base_path, folder_name, file_name)\n"
      ],
      "metadata": {
        "id": "91z1log56SBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d87a8e-1009-4896-a5bf-599b38c07c57"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of image_paths:\", len(image_paths))\n",
        "print(\"Length of labels:\", len(labels))"
      ],
      "metadata": {
        "id": "e_YeUe-3coLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15de8e17-47cf-43ee-b9da-6daea671ddfe"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of image_paths: 300\n",
            "Length of labels: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# データセットの分割とデータローダーの作成\n",
        "for fold, (train_index, val_index) in enumerate(kfold.split(image_paths), 1):\n",
        "    print(f'==========Cross Validation Fold {fold}==========')\n",
        "    image_directory = '/content/drive/MyDrive/lernig/OBI_ref/image'\n",
        "    label_directory = '/content/drive/MyDrive/lernig/OBI_ref/label'\n",
        "\n",
        "\n",
        "    image_paths, labels = update_xml_paths(image_directory, label_directory)\n",
        "    train_paths = [image_paths[i] for i in train_index]\n",
        "    train_labels = [labels[i] for i in train_index]\n",
        "    val_paths = [image_paths[i] for i in val_index]\n",
        "    val_labels = [labels[i] for i in val_index]\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, transform=data_transform['train'])\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, transform=data_transform['val'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    dataloaders  = {\"train\":train_loader, \"val\":val_loader}\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # 事前学習済みモデルの全結合層の出力ユニット数をデータのクラス数に置き換え\n",
        "    num_classes = len(set(train_labels))\n",
        "    net.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "# ここでデータローダーを使用して学習や検証を行う\n",
        "    data_size = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "    train_model(model, criterion, optimizer, num_epochs)\n",
        "    print('Train done.')\n",
        "    file_path = os.path.join(directory_path, f'{file_name}_{str(fold)}.pth')\n",
        "    # モデルの保存\n",
        "    torch.save(model.state_dict(), file_path)\n",
        "    print('Train saved.')\n",
        "    model.load_state_dict(torch.load(file_path))\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for images, labels in val_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "      print('Accuracy of the network on the test images: %d %%' % (100 * correct / data_size['val']))\n",
        "\n"
      ],
      "metadata": {
        "id": "abuhpmzo4MCL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b82036c6-aaa4-4314-a3e9-e33f169c4b76",
        "collapsed": true
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========Cross Validation Fold 1==========\n",
            "trainLoss: 0.6001 Acc: 0.7292\n",
            "valLoss: 0.5453 Acc: 0.7667\n",
            "trainLoss: 0.5825 Acc: 0.7417\n",
            "valLoss: 0.5339 Acc: 0.7667\n",
            "trainLoss: 0.5379 Acc: 0.7500\n",
            "valLoss: 0.5154 Acc: 0.7667\n",
            "trainLoss: 0.5285 Acc: 0.7458\n",
            "valLoss: 0.4977 Acc: 0.7667\n",
            "trainLoss: 0.5197 Acc: 0.7542\n",
            "valLoss: 0.5008 Acc: 0.7667\n",
            "trainLoss: 0.5096 Acc: 0.7583\n",
            "valLoss: 0.4903 Acc: 0.7833\n",
            "trainLoss: 0.5043 Acc: 0.7625\n",
            "valLoss: 0.4646 Acc: 0.8000\n",
            "trainLoss: 0.5225 Acc: 0.7583\n",
            "valLoss: 0.4598 Acc: 0.8000\n",
            "trainLoss: 0.5064 Acc: 0.7333\n",
            "valLoss: 0.4613 Acc: 0.8667\n",
            "trainLoss: 0.4944 Acc: 0.7542\n",
            "valLoss: 0.5124 Acc: 0.8167\n",
            "trainLoss: 0.4837 Acc: 0.7833\n",
            "valLoss: 0.4656 Acc: 0.8667\n",
            "trainLoss: 0.4903 Acc: 0.7667\n",
            "valLoss: 0.4743 Acc: 0.8667\n",
            "trainLoss: 0.4987 Acc: 0.7708\n",
            "valLoss: 0.4457 Acc: 0.9000\n",
            "trainLoss: 0.4861 Acc: 0.7875\n",
            "valLoss: 0.4402 Acc: 0.9167\n",
            "trainLoss: 0.4765 Acc: 0.7875\n",
            "valLoss: 0.4065 Acc: 0.8833\n",
            "trainLoss: 0.4675 Acc: 0.7833\n",
            "valLoss: 0.4090 Acc: 0.9000\n",
            "trainLoss: 0.4690 Acc: 0.7792\n",
            "valLoss: 0.4739 Acc: 0.8333\n",
            "trainLoss: 0.4577 Acc: 0.8000\n",
            "valLoss: 0.4008 Acc: 0.8833\n",
            "trainLoss: 0.4435 Acc: 0.7750\n",
            "valLoss: 0.4139 Acc: 0.8667\n",
            "trainLoss: 0.4326 Acc: 0.8125\n",
            "valLoss: 0.4322 Acc: 0.8500\n",
            "trainLoss: 0.4653 Acc: 0.8000\n",
            "valLoss: 0.3931 Acc: 0.8500\n",
            "trainLoss: 0.4585 Acc: 0.7875\n",
            "valLoss: 0.3888 Acc: 0.8833\n",
            "trainLoss: 0.4226 Acc: 0.8333\n",
            "valLoss: 0.4562 Acc: 0.8333\n",
            "trainLoss: 0.4362 Acc: 0.8125\n",
            "valLoss: 0.3963 Acc: 0.8500\n",
            "trainLoss: 0.4270 Acc: 0.7917\n",
            "valLoss: 0.4625 Acc: 0.8333\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-7f264ffb8034>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# ここでデータローダーを使用して学習や検証を行う\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{file_name}_{str(fold)}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-b004af643e23>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-6e4a1d142f5f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrUub5RbVWvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}